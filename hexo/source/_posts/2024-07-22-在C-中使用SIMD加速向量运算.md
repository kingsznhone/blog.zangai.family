---
title: 在C#中使用SIMD加速向量运算
date: 2024-07-22 04:46:45
tags: 
  - .NET
cover: /img/SIMD.png
---

# 可用性概述
.NET 7 引入了一系列贴近底层的方法，给.NET百宝箱里又增加了一些优化性能的工具。

尤其是Vector256中的一些新方法，对向量的操纵下沉到了指针级。

如今的CPU已经全面支持AVX2指令集，AVX2指令集的位宽是256bit。那还等什么，`Vector256<T>`，开冲

以前如果想要使用Vector256，需要手动加载每个部分的变量，比如这个方法

``` c# 
public static System.Runtime.Intrinsics.Vector256<float> Create (float e0, float e1, float e2, float e3, float e4, float e5, float e6, float e7);
```

更有甚者
 
``` c# 
public static System.Runtime.Intrinsics.Vector256<byte> Create (byte e0, byte e1, byte e2, byte e3, byte e4, byte e5, byte e6, byte e7, byte e8, byte e9, byte e10, byte e11, byte e12, byte e13, byte e14, byte e15, byte e16, byte e17, byte e18, byte e19, byte e20, byte e21, byte e22, byte e23, byte e24, byte e25, byte e26, byte e27, byte e28, byte e29, byte e30, byte e31);
```
这东西确实有点精神污染了，C++里把数据加载到ymm中， 只需要给一个数组的指针就可以了，用这32形参的方法创建一个SIMD对象,不知道要多出来多少新对象的分配开销

也许这些克苏鲁方法就是为了测试SIMD做的铺垫，直到.NET 7，终于可以像C++一样玩SIMD了。

为了最大限度减少虚拟机里的开销，需要直接干进底层，从内存指针中直接读取数据

假设有一个浮点数组`float[] data`，想要创建它的数组指针，只需要调用

```
ref float ref_data = ref MemoryMarshal.GetReference(data);
```

就可以直接从内存中把数据加载到ymm寄存器上了

```
Vector256<float> v = Vector256.LoadUnsafe(ref ref_data, (nuint)Offset);
```

`Vector256<T>`重载了一系列常见的运算符，你可以用它做加减乘除左移右移之类的操作，对于大量数值数据的四则运算操作有奇效。

# Benchmark

说起性能测试，那就祭出最经典的数列求和

测试数据为长度1M的数组，随机填充

跑分框架使用BenchmarkDotNet

构建运行时为.NET 8 Release

基准方法为小学生都会写的循环求和

``` c#
public T SumIntegerArray<T>(Span<T> data) where T : INumber<T>
{
    T sum = T.Zero;
    for (int i = 0; i < data.Length; i++)
    {
        sum += data[i];
    }
    return sum;
}
```

循环只需要一路for下去就行了，SIMD方法需要考虑的就多了

首先就是边界问题，如果数组长度不能凑整，就需要把余下的元素放到最后使用传统方法求和，加到总结果里

为此需要通过调用Vector256<T>.Count获取数据类型在ymm中所能容纳的最大数量，就可以进行边界检查了

```
public T SumIntegerArray_SIMD<T>(Span<T> data) where T : INumber<T>
{
    T sum = T.Zero;
    int vectorSize = Vector256<T>.Count;
    Vector256<T> v_sum = Vector256<T>.Zero;
    ref T ref_data = ref MemoryMarshal.GetReference(data);
    int SIMDLength = (data.Length - vectorSize);
    int Offset = 0;
    for (Offset = 0; Offset <= SIMDLength; Offset += vectorSize)
    {
        var vc = Vector256.LoadUnsafe(ref ref_data, (nuint)Offset);
        v_sum += vc;
    }
    sum = Vector256.Sum(v_sum);
    for (; Offset < data.Length; Offset++)
    {
        sum += data[Offset];
    }
    return sum;
}
```

#### 跑分-FP32

```

BenchmarkDotNet v0.13.12, Windows 11 (10.0.22631.3880/23H2/2023Update/SunValley3)
12th Gen Intel Core i9-12950HX, 1 CPU, 16 logical and 16 physical cores
.NET SDK 8.0.303
  [Host]   : .NET 8.0.7 (8.0.724.31311), X64 RyuJIT AVX2 [AttachedDebugger]
  .NET 8.0 : .NET 8.0.7 (8.0.724.31311), X64 RyuJIT AVX2

Job=.NET 8.0  Runtime=.NET 8.0  

```
| Method       | Mean      | Error    | StdDev   | Ratio | Allocated | Alloc Ratio |
|------------- |----------:|---------:|---------:|------:|----------:|------------:|
| Compute      | 473.89 μs | 2.651 μs | 2.350 μs |  1.00 |         - |          NA |
| Compute_SIMD |  61.03 μs | 0.496 μs | 0.464 μs |  0.13 |         - |          NA |

使用SIMD以后，所用时间来到了标量求和的大约八分之一，非常令人满意的性能提升

接下来再测一测别的数据类型

#### 跑分-Int32

| Method       | Mean      | Error    | StdDev   | Ratio | Allocated | Alloc Ratio |
|------------- |----------:|---------:|---------:|------:|----------:|------------:|
| Compute      | 298.51 μs | 3.477 μs | 3.253 μs |  1.00 |         - |          NA |
| Compute_SIMD |  55.68 μs | 0.829 μs | 0.735 μs |  0.19 |         - |          NA |

有意思的结果出现了，使用传统计算的速度更快了，普通浮点运算需要调用SSE，而整形运算直接怼进ALU，延迟和缓存的表现都好了不少

SIMD的性能还是遥遥领先

#### 跑分-Int64

| Method       | Mean     | Error   | StdDev  | Ratio | Allocated | Alloc Ratio |
|------------- |---------:|--------:|--------:|------:|----------:|------------:|
| Compute      | 302.3 μs | 1.63 μs | 1.52 μs |  1.00 |         - |          NA |
| Compute_SIMD | 134.5 μs | 0.79 μs | 0.70 μs |  0.45 |         - |          NA |

依然是一个很有意思的结果，Int32和Int64的运算速度没有变，而SIMD变慢了一半。

整形不管是32位还是64位，用的都是同样的寄存器，同样的指令数量。而YMM宽度限制导致每次只能加载4个Int64。

#### 跑分-FP64

| Method       | Mean     | Error   | StdDev  | Ratio | Allocated | Alloc Ratio |
|------------- |---------:|--------:|--------:|------:|----------:|------------:|
| Compute      | 474.6 μs | 3.38 μs | 2.83 μs |  1.00 |         - |          NA |
| Compute_SIMD | 134.9 μs | 2.46 μs | 2.30 μs |  0.28 |         - |          NA |

和FP32的跑分对比也可以发现Int32/64的对比相同的结论。

传统数据都跑了一遍，那就再整点活吧。

#### 跑分-Int16

| Method       | Mean      | Error    | StdDev   | Ratio | Allocated | Alloc Ratio |
|------------- |----------:|---------:|---------:|------:|----------:|------------:|
| Compute      | 472.34 μs | 3.192 μs | 2.986 μs |  1.00 |         - |          NA |
| Compute_SIMD |  30.46 μs | 0.410 μs | 0.383 μs |  0.06 |         - |          NA |

ymm寄存器的宽度优势进一步被放大，相比Int32，传统派速度反而变慢了，这大概是Int16类型在实际运算中，还要转换成Int32导致的额外开销

都什么年代了，还在用short?

你别说还真有游戏用Short，弹丸论破游戏很多数据用的就是Short存储，小鬼子的编程思维恐怕还停留在红白机时代

#### 跑分-Byte

最后给大家整个大活

| Method       | Mean      | Error    | StdDev   | Ratio | Allocated | Alloc Ratio |
|------------- |----------:|---------:|---------:|------:|----------:|------------:|
| Compute      | 468.97 μs | 2.273 μs | 2.015 μs |  1.00 |         - |          NA |
| Compute_SIMD |  12.05 μs | 0.233 μs | 0.269 μs |  0.03 |         - |          NA |


ymm再一次凭借宽度优势夺得了宝座，传统派还是因为额外的转换开销，性能水平掉到了和short相同的级别

#### 跑分-FP16

我没有支持AVX512的CPU，没法跑这个玩意。而且目前.NET源码里对AVX512相关的构建还没有完成。

FP16和BF16都是AVX512专属的指令集，还细分了好几个子集，就算是按摩店的家用级CPU也不一定支持这些指令集。

都FP16和BF16了，那为什么不直接pytorch启动？